# 機械学習関連でメモ的に書き溜めた用語集です。
- 個人的に機械学習を勉強し始めて見聞きしたものを書き留めていったものです。
- 書き留めた時点で忘却ゲート活性化しているため、聞かれてもほとんど答えられないかもです。

----------------------------------------
# DNNの種類

http://gagbot.net/machine-learning/ml4

- CNN(Convolution Neural Network): 通常のNeural NetworkにConvolutionを追加したもの。ここでは、Convolution、畳み込みとは一体なんなのか、という点と、なぜそれが画像認識に有効なのかについて説明していきます。
    CNNは、フィルタ内の領域の情報を畳み込んで作成するConvolution Layerを導入した、Neural Networkのことである
    Convolution Layerはフィルタを移動させながら適用することで作成し、フィルタの数だけ作成される。これを重ねて活性化関数(ReLU等)で繋いでいくことで、ネットワークを構築する。
    畳み込みにより点ではなく領域ベースでの特徴抽出が可能になり、画像の移動や変形などに頑健になる。また、エッジなど領域ベースでないとわからない特徴抽出も可能になる。

- RNN（Recurrent Neural Network）:CNNが扱う画像データは二次元の矩形データでしたが、音声データは可変長の時系列データです。この可変長データをニューラルネットワークで扱うため、隠れ層の値を再び隠れ層に入力するというネットワーク構造にしたのが、RNN（Recurrent Neural Network）です。このRNNには、長時間前のデータを利用しようとすると、誤差が消滅したり演算量が爆発するなどの問題があり、短時間のデータしか処理できませんでした。

- LSTM:LSTM（Long Short-Term Memory）はRNNの欠点を解消し、長期の時系列データを学習することができる強力なモデルです。発表されたのは1997年とかなり前ですが、ディープラーニングの流行と共に、最近急速に注目され始めたモデルです。自然言語処理に応用される、大きな成果をあげ始めています。

- オートエンコーダ（自己符号化器）:オートエンコーダの核は次元削減である。オートエンコーダはニューラルネットワークの一種で、情報量を小さくした特徴表現を獲得するためにある。圧縮していく過程をエンコーダと呼び、復元する過程をデコーダと呼ぶ。オートエンコーダを用いてパーセプトロンの重みの初期値を予め推定しておくことを事前学習(pre-training)というが、もう今では実用的な用途としてはめったに使われてないらしい。オートエンコーダは次元圧縮することを目的としているが、入力と出力を同じにする半教師あり学習で対象の特徴表現を自己学習してしまうところがポテンシャルを感じさせる。そして、今は生成モデルでも効果を発揮しはじめている。

- GAN: 最新の生成モデルであるGAN（Generative Adversarial Network）は、非常に注目を集めている手法です。GANの基本的な考え方はシンプルなので、たとえ話で説明します。図のイラストのように、ニセ札造りの偽造者と警察官の2名の登場人物がいるとします。偽造者は、本物の紙幣と似たニセ札を造ります。警察官は、ニセ札を見破ろうとします。下手なニセ札は簡単に警察官に見破られますが、偽造者の腕が上がって精巧なニセ札になっていくと、警察官もなんとかニセ札を見破ろうと頑張って見分けようとします。お互いに切磋琢磨していくと、最終的にはニセ札が本物の紙幣と区別がつかなくなるでしょう。

- SNN(Spiking Neural Network):従来のニューラルネットワークとは異なり発火頻度ではなくニューロンの内部電位に注目したモデル。アナログ量をパルスタイミングで表現するため、生体模倣の忠実度が高い。非同期動作なので高速処理が課題。 

- NNA(Neural Network Accelerator): 専用HW。基本行列積和算の高速化。


------------------------------------------------
# 機械学習FW・ライブラリ

- Torch: Define-by-Run で動的にニューラルネットワークをするパラダイム，老舗ライブラリ。PyTorchが話題？

- Caffe: [C++(python,matlabも可能?)]. 画像認識に向いている？C++で実装され、GPUに対応しているため、高速な計算処理が可能。Caffe is a community　というキャッチコピーもあるほど、その開発コミュニティーが活発にgithubを更新していたり、サンプルコードも多く初心者に推奨される。大規模画像認識のコンテストILSVRCで2012年に1位となった畳み込みニューラルネットワークの画像尾分類モデルがすぐに利用できるようになっている。Caffeは、カリフォルニア大学バークレー校のコンピュータビジョンおよび機械学習に関する研究センターであるBVLCが中心となって開発している。ヤフージャパンは2014年6月から同センターのスポンサーになっていて、Caffeの開発を含めたセンターの研究の支援を行っている。
- Caffe2: C++.Facebook?　Caffeは、カリフォルニア大学バークレー校のコンピュータビジョンおよび機械学習に関する研究センターであるBVLCが中心となって開発している。

- Chainer: [Python].ニューラルネットワーク記述に特化しているライブラリ。プリファードネットワークス社の日本産であり、非常に扱いやすい記述体系を持っています。Define by Runによって柔軟な計算グラフの構築が可能であることが、最も大きな特徴。Pythonだが、計算の処理はnumpy（C言語での実装）によって行われる。主な需要は国内のみ。日本製らしく省メモリ指向？

- Keras: ★TensorFlowの上位APIに統合される?★ 層をただただ積み重ねていくだけという、とんでもなく簡単な実装が可能です。Kerasの登場により、Chainerを使っていた人たちが一気に流れていった印象さえあります。Kerasは「Theano」や「TensorFlow」のようなテンソルを高速計算するライブラリのラッパーとして登場しました。記述を簡便化するのが役割であるため、簡単に計算グラフを構築できるという点は当たり前と言えば当たり前です。計算グラフの構築が簡単なだけでなく、学習のコードも、学習回数などの条件を引数にたった一行で実装できてしまいます。

- TensorFlow :[Python/C++].実際には多次元配列の計算を効率化し、計算グラフとして実行することがTensorFlowの役割です。ニューラルネットは計算グラフで記述できるため、当然TensorFlowが深層学習が大活躍するのは疑いの余地がありません。しかし実際には、深層学習に偏ったものではなく、もっと広く一般的な計算を行うことができるフレームワークとなっています。深層学習の発展によって、TensorFlow側も、深層学習の実装をサポートするべく様々な関数を追加していますが、記述のしやすさという点において他のフレームワークに勝っていたかといえばそうでも無いというのが今までの印象でした。Define and Runのため、学習中の計算グラフの変更不可がデメリット。GPU/メモリをあるだけ使うので重い？

- Theano: [Python] モントリオール大学. 機能としては、ディープラーニングの他に「行列演算」「実行時にCコードを生成してコンパイル」「自動微分」「GPU処理(要CUDA)」もあり、ケースによっては数値計算ライブラリ「Numpy」よりも高速に計算できる。Deep learning に関するTutorialの量がとても多い。Theano自体は自動偏微分機能・GPU対応などをサポートする計算ライブラリで、Deep learning専用のパッケージではない。自分で理論から理解してScratchで実装したい人にはとても参考になる。Theanoをベースに開発されたライブラリが多いようだ。

- TensorFlow XLA: XLA (Accelerated Linear Algebra 直訳すれば、加速された線形代数) 。ユーザーが作成した TensorFlow グラフを JIT コンパイル技術により実行時に分析します。ランタイムにおける実際の次元や型に応じて特化したグラフをを生成し、複数の演算をまとめて、CPU や GPU、カスタム アクセラレータ（Google の TPU など）などで効率的に実行できるバイナリコードを生成します。tfcompileを使ってTensorFlow表記をCPUの実行コード(TensorFlow Runtime不要)に変換できる。

- PyTorch: Facebookがプロト用に推進(ONNX経由でCaffe2で製品化)。2017初頭から人気爆発。Define by RunでChainerが終わった？

- cuDNN:NVIDIAが公開しているDeep Learning用のライブラリ

- core ML: Apple

- ARMComputeLibrary

- sonnet: DeepMind社のTensorFlow-based neural network library 

- Protocol Buffers: Node間のデータ交換に使われる。シリアライズされる。NNのデータでもよく使われる。tfliteはFlatBuffersらしい。

- ONNX(Open Neural Network Exchange): AI モデルのためのオープンソース・フォーマット.MicrosoftとFacebookが提案。tensorflowへはコンバータ対応。学習済データは含まない？？？

- NNEF（Neural Network Exchange Format）: Khronos Groupが2017/12/25に1.0(暫定仕様)を発表。ニューラルネットのファイル形式の違いを吸収するフォーマット。ONNXとの違いはテキストと非営利団体主導？OpenVXとも連携。

- OpenVX: Khronos Group。画像認識のAPIへの標準化。OpenXRと共にVR, Vision, NN関連。

- NHWC: Num_samples x Height x Width x ChannelsのData formats, NHWC is the TensorFlow default and NCHW is the optimal format to use when training on NVIDIA GPUs using cuDNN. パフォーマンス的には、NHWCとNCHWの両対応が望ましい？

-----------------------
# 機械学習指標の用語
http://www.procrasist.com/entry/ml-metrics
二値分類のlogloss(Logarithm Loss)は、分類の結果だけを見るのではなく、各クラスに属する確立(分類器の自信?)のlog値の平均で、分類に至った過程まで見る

関数近似：NNを学習して関数の振る舞いを近侍すること

- Accuracy 	正解率のこと。予測結果全体と、答えがどれぐらい一致しているかを判断する指標。★論文では使わない
- Precision 	適合率/精度のこと。予測を正と判断した中で、答えも正のもの。★再現率とともに情報検索からの用語。片方固定で使う場合も？
- Recall 	再現率のこと。答えが正の中で、予測が正とされたもの。
- F-measure 	F値のこと。予測精度の評価指標。PresicionとRecallの調和平均。★語源はE値の記憶違い？https://ci.nii.ac.jp/naid/110002939532
適合率のマイクロ平均、マクロ平均：マイクロが足したあと適合率、マクロが適合率の平均 ★マクロは経済学では各国（カテゴリ)ごとの指標を見るようにマクロな単位で見る。マイクロは国(カテゴリ)を無視して細かい単位で見る？
- 混同行列(confusion matrix):True/False Positive/Negative　★文字認識だと行列数が多く、本当に混同する状況を示す
- 平均二乗誤差(Root mean squared error,RMSE): ★二乗なのは標準偏差とのからみ。
- 決定係数(Coefficient of Determination): R2とも呼ばれる。0-1で1に近いほど良い。★分子が全問正解で0になる。分母は1にする正規化。

# 混同行列(confusion matrix)
True/Falseは形容詞で、例えば正解データが正であるものはTPとFNである点に注意。
    True Positive(TP)： 正解データが正であるものを、正しく正と予測できた数
    False Positive(FP)：正解データが負であるものを、間違って正と予測した数
    Flase Negative(FN)：正解データが正であるものを、間違って負と予測した数
    True Negative(TN)：正解データが負であるものを、正しく負と予測できた数
False Positive Rate(偽陽性率）：FP/(FP+TN)
	「偽陽性率」は、正解データが負であるものを間違って正と予測した割合。(分母が正解データ負の総和)
True Positive Rate(真陽性率）：TP/(TP+FN)
	「真陽性率」は、正解データが正であるものを正しく正と予測した割合。(分母が正解データ正の総和)

# ROC曲線
False Positive Rate（偽陽性率）を横軸にTrue Positive Rate（真陽性率）を縦軸に置いてプロットしたもの
- ROC曲線の特徴
	ROC曲線は右にいくほど下がることはない
	偽陽性率の値が小さい時点で、高い真陽性率を達成しているモデルほど良い
- AUC(Area under an ROC curve):ROC曲線とx軸y軸で囲まれた部分（下図の斜線部）の面積ができるだけ大きいものほど良いモデルであると言えそうです。この面積の値がAUC(Area under an ROC curve)となります。AUCが1に近いほど性能が高いモデルとなり、完全にランダムに予測される場合、AUCは0.5、つまりROC曲線は原点(0,0)と(1,1)を結ぶ直線になります。

# リフトチャート（LiftChart）
フトチャートには複数の定義があり、DataRobotで使われているバージョンを定義する文献はほとんどありません(累積反応曲線と言われる別のものをリフトチャートと呼ぶことがある)。予測モデルの精度をはかるために使われます。モデルの出力する予測値がどれくらいの判別能力や予知能力を有しているのか、また複数のモデルを比較した時に、どちらのモデルの精度が良いのかを素早く視覚的に捉えることができます。
    実測の線の角度 – 一般的には、急なほどよい
    予測と実測の近しさ – 一般的には、近いほどよい
- 累積反応曲線/Cumulative Response Curveと呼ばれることもあるようである。ROCとの違いは、縦軸は真陽性率/True Positive Rate(TPR)と同じままだが、横軸を陽性予測率/Positive Prediction Rateとする。

# アンサンブル学習
https://www.codexa.net/what-is-ensemble-learning/
性能の低い学習器を組み合わせて、高性能な学習器を作る方法. 以下の３つある。分類は各モデルの予測を最終的に多数決、回帰は平均値を使うのが一般的。
	- バギング: 弱学習器を並列に学習して組み合わせる⼿法。bagging = bootstrap(ブートストラップ) + aggregating(集約)。ランダムフォレストはこれ。一般的にモデルの予測結果のバリアンスを低くする特徴があります。
	- ブースティング: 弱学習器を順番に学習して組み合わせて強くしていく⼿法. 前の学習器が誤分類したデータを優先的に正しく分類できるように学 習していく.一般的にモデルの予測精度に対してバイアスを下げる特徴があります。XgBoostはこれ。
	- スタッキング: モデルを積み上げていく方法で高度・複雑。上手く利用することによりバイアスとバリアンスをバランスよく調整する事が可能.
- 弱学習器(性能の低い学習器) 𝑓(𝑥) • 正解率が0.5より⼤きい • = ランダムに+1か-1を返すものよりは良いもの • 正解率が0.5より⼩さい学習器は分類結果を反対にすれば、正解率が 0.5より⼤きいものを作れる • 注意 • 複雑な学習器も使えるが計算量の⼩さい単純な学習器がよく使われる • 決定株、決定⽊など • 全く同じ分類器では性能は上がらない(多様性が必要
- ブートストラップ N個のデータから重複を許してランダムにN回データを選ぶこと。統計の分野で⺟集団の統計量の推定に使われてきた⼿法。ちょっとだけ違うデータセットをたくさん作れる。
- 「バイアス（Bias）」と「バリアンス（Variance）」:バイアスは実際値と予測値との誤差の平均のことで、値が小さいほど予測値と真の値の誤差が小さいということになります。対してバリアンスは予測値がどれだけ散らばっているかを示す度合いのことで、値が小さいほど予測値の散らばりが小さいということになります。バイアスとバリアンスはトレードオフの関係にある

# ブースティング
データの一部を抽出してそれで弱学習機を作り、最後に合わせるのはバギングと同様。違いは前回の結果を利用するのがブースティング。なので並列処理はできない。
- アダブースト:前回の結果で誤分類された値の重みを大きくするように更新する。ブースティングと単に言えばこれを指すことが多いらしい。
- マダブースト・ロジットブースト:アダブーストの損失関数が違うバージョンみたい。『イラストで学ぶ機会学習』に出ていたが他では見た記憶がない。
- 勾配ブースティング:勾配降下法を使ったブースティングのこと。正直なところアダブーストとの違いがいまいちよくわからない。
- GBDT(Gradient Boosting Decision Tree):弱学習機に決定木を使った勾配ブースティングのこと。かっこよく聞こえる。
- XgBoost: これはGradient Boosting（勾配ブースティング）の高速なC++実装S．従来使われてたgbtより10倍高速らしい

# 機械学習で使われるシリアライズ形式
https://www.sambaiz.net/article/46/
- MessagePack: JSONのように使うことができ、速くてサイズが小さい。
- ProtocolBuffers: インタフェース定義言語 (IDL) で構造を定義する通信や永続化での利用を目的としたシリアライズフォーマットであり、Googleにより開発されている。gRPCでも使われる。message typeをprotoファイルで定義する(proto3)。このファイルから各言語のコードを生成することができる。pbでGoogleのあらゆるサービスやtensorflowで使用されている
- FlatBuffers: Googleの、ゲームなどパフォーマンスを要求するアプリケーションのためのシリアライズフォーマット。 データにアクセスする前にparseやunpackする必要がなく、オブジェクトごとのメモリ割り当てが必要。protoの代わりにschemaファイルを書く。tensorflow liteで使用されている。

---------------------
# 学習の種類

- 教師アリ学習(supervised learning):判断の結果がわかった状態で実施
- 教師ナシ学習(unsupervised learning):判断の結果がわからない状態で実施…データの構造を読み取る…マイニング要素。データマイニング
- 強化学習(reinforcement learning):データから判断をして行動すると、その行動が評価されるという仕組みを使って、判断を改善していく.例えばQ学習
- 半教師アリ学習(semi-supervised learning):教師アリと教師ナシを併せる。正答のあるデータと、ないデータの両方を用いて学習を行う半教師あり学習は、近似関数や分類器を生成するのに適しており、データ収集のためのコストが低くてすむという意外な利点もあります。半教師あり学習のなかで、特に与えられたデータの中のラベルがないものに関してのみ、予測を行うトランスダクションという学習法もあります。
- トランスダクション(transduction. transductive inference):観測された具体的な（訓練）例から具体的かつ固定の（テスト）例の新たな出力を予測しようとする。演繹(deduction)と帰納(induction)と、Transduction。Deductionは入力からルールをみつけること。
- Learning to learn:複数の判断を一緒に行うことで判断作業全体の改善を目指す

- 深層強化学習: 自動運転など試行に対しフィードバックを与え(強化学習)る深層学習。

- バックプロパゲーション（Backpropagation): 誤差逆伝播法 backwards propagation of errorsの略。エラー(および学習)は出力ノードから後方のノードへと伝播する。技術的に言えば、バックプロパゲーションはネットワーク上の変更可能な重みについて、誤差の傾斜を計算するものである[11]。この傾斜はほとんどの場合、誤差を最小にする単純なアルゴリズムである確率的最急降下法で使われる。バックプロパゲーションを行う場合、ネットワークは少なくとも三層以上でなければならない（入力層、中間層、出力層）。また、多層ネットワークの中間層が意味のある関数を表すには、非線形の活性化関数でなければならない。線形な活性化関数の多層ネットワークは、単層ネットワークと等価である。非線形の活性化関数としては、ロジスティック関数(中でも tanh などのシグモイド関数)、ソフトマックス関数、ガウス関数などが一般的であったが、中間層の活性化関数としては現在は max(x, 0) が最善であるとされている

- 勾配降下法::optimizer 「最適化アルゴリズム」の一種。 
    - 最急降下法(Gradient Descent):すべての誤差の合計を取ってからパラメタを更新。計算が遅い
    - 確率的勾配降下法（Stochastic Gradient Descent - SDG）学習データの中からランダムに1つを取り出して誤差を計算、パラメタを更新。計算が早い
    - ミニバッチ確率的勾配降下法（Minibatch SGD - MSGD）:上二つの間を取ったような形で、学習データの中からランダムにいくつかのデータを取り出して誤差を計算、パラメタを更新

- 最適化アルゴリズム：
     - 「スケーリングされた共役勾配」。共役勾配法の使用を正当化する仮定は、バッチ学習タイプにのみ適用されるため、この方法はオンライン学習またはミニバッチ学習では使用できません。  
    - 「勾配降下」。この方法は、オンライン学習またはミニバッチ学習で使用する必要があります。また、バッチ学習で使用することもできます。

- epoch: 「学習率の減衰 (エポック)」:オンライン学習またはミニバッチ学習で勾配降下が使用される場合、初期の学習率を学習率の下限まで減少させるために必要な学習サンプルのエポック数 (p) またはデータ・パス数です。

- 活性化関数(activation function)：ニューラルネットワークにおいて、線形変換をした後に適用する非線形関数もしくは恒等関数のこと。ステップ関数、線形結合、シグモイド関数、ソフトサイン、ソフトプラス、ReLU（ランプ関数.max(0, x) ）

- ロジスティック回帰(Logistic regression)：ベルヌーイ分布に従う変数の統計的回帰モデルの一種である。連結関数としてロジットを使用する一般化線形モデル (GLM) の一種でもある。モデルは同じく1958年に発表された単純パーセプトロンと等価であるが、scikit-learn などでは、パラメータを決める最適化問題で確率的勾配降下法を使用する物をパーセプトロンと呼び、座標降下法や準ニュートン法などを使用する物をロジスティック回帰と呼んでいる。

- バッチ学習：学習データxがN個あるときに、N個のデータを全て用いて、それぞれのデータでの損失lの平均を計算し、それをデータ全体の損失Lと考え、L(t,x;w)=1N∑i=1Nl(ti,xi;w)と定義して学習に臨むのがバッチ学習です。一般的に学習は安定しており、かつ、ニューラルネットに入れたデータは事実上計算も同時に行えるため、以下2つの方法に比べ高速です。
- オンライン学習：一方で、N個のデータx1,x2,...,xNからランダムに１つxiを選び出し、そのデータ１つに対する損失lをそのままLに用いてL(t,x;w)=l(ti,xi;w)と損失関数を定義して学習に臨むのが、オンライン学習、あるいは「確率的勾配法」と言います。通常、この学習はあまり安定しません。
- ミニバッチ学習：全体を考慮したバッチ学習と、確率的勾配法の間を取ったのがミニバッチ学習であり、このとき学習データxがN個あるときに、ランダムなn(?N)個のデータを使いL(t,x;w)=1n∑i=1nl(ti,xi;w)を損失関数と定義し、学習を行います。実際は多くの場面でこの方法が用いられており、「確率的勾配法」と言う時には、もはやこちらのミニバッチ学習を指すことのほうが今では多いです。

- ラベル: 教師あり学習でデータについてる答え。


---------------------------------------------------------------------
# HW
TPU/Clould TPU：初代TPUは推論専用。Cloud TPUになって学習にも対応

NPU: Haiwai AIプロセッサ

Neural Engine: Apple A11 Bionic

Qualcommは、GPUでは無くDSP使用？

Pixel Visual Core: GoogleがPixel 2に搭載。HDR＋品質。Pixel Visual CoreはGoogleが設計した8個の「Image Processing Unit（IPU）」を搭載する。このIPUはMITが開発したオープンソースの画像処理向け言語「Halide」とGoogleの機械学習ライブラリ「TensorFlow」をサポートし、今後は画像処理以外の機能でも使っていく計画。

●センサー
ToF(Time of Flight):投射したレーザーが対象まで往復するのにかかる時間（パルスの位相差）から距離を計測

RiDAR(Light Detection and Ranging): レーザー光を使ったレーダー。レーザーを連続的に発射し、その反射点の三次元位置を高密度にしかも安価に測定するシステムである。Google Carをはじめとする自動運転車に付いているセンサーの中で、ひときわ目を引くのが、このLIDARでしょう。パトカーの回転灯のようにぐるぐると屋根の上で回転するため、非常に目立つのです。このLIDARは周囲360度の3D空間構造を瞬時に読み取ってデータ化するためのセンサーです。原理は「time-of-flight」方式を採用している2代目Kinectと同じです。自動運転ためのセンサーとしてLIDARが優れているのは、周囲の3D空間を素早く正確に把握可能であることだけではありません。好都合なことに、LIDARは、道路に引かれている白線に代表されるレーンマーカーを読み取ることもできます。というのも、道路の白線は高い反射率を持つ塗料で塗られていることが多いため、LIDARは、白線と路面の反射率の違いを3Dデータに含めることが可能なのです。

ミリ波レーダー:電波を使うミリ波レーダーは、分解能はLIDARよりも劣るものの、天候に関わらず検知可能で、その範囲も250mと広い。

TrueDepth: 「iPhone X」の前面上部に搭載されたフロントカメラや赤外線カメラ、環境光センサー、近接センサー、ドットプロジェクターなどを組み合わせたカメラシステム。ユーザーが目を開いて画面を見つめているのを確認したら、3万以上の赤外線ドットを顔に投影し、このデータを解析することで顔の深度マップ、および2Dの赤外線画像を作成。

DMI（Distance Measuring Instrument）:走行距離計：タイヤの回転を数えることで、車がどれだけ進んだかを測定するセンサー（走行距離計）です。

IMU（Inertial Measurement Unit）:慣性航法のための6軸加速度センサー



-------------------

# EDA: 研究フェーズ。探索的データ分析（Exploratory Data Analysis）の頭文字

# Vanilla LSTM: 古典的なLSTM

# スパースモデリング、正則化
リッジ回帰、Lasso回帰、Elastic Net
https://aizine.ai/ridge-lasso-elasticnet/#toc8


# カーネル法
- 概要、２つの捉え方
https://su-butsu-kikaigakusyuu.hatenablog.com/entry/2018/04/21/122541
- カーネル関数で写像の内積計算だけで済む＝＞カーネルトリック
https://www.hellocybernetics.tech/entry/2016/08/09/051355


# 勾配法
- 座標降下法と最急降下法
- ミニバッチと確率勾配法 => 局所解に回避する可能性あり

# 特異値分解SVDと低ランク行列近似
特異値分解されたもののうち、上位のランクだけ選んで再構成すれば低ランク近似になります。
低ランク行列の再構成とは異なる？
低ランク行列再構成も内部的には特異値分解をしている
未知のN1×N2行列X0に対して，線形観測をM回行うものとし，観測結果をy= (y1, y2, . . . , yM)T,yi=⟨Ai, X0⟩,i= 1,2, . . . , Mとする．ただし，⟨X, Y⟩=tr(XTY)である．観測行列{A1, A2, . . . , Am}は既知であるものとし，観測を線形作用素A:RN1×N27→RMを使ってy=AX0と表すことにする．未知の行列X0の階数rankX0が小さいことが分かっているとき，yとAとからX0を推定するのが，ここでの行列再構成の問題である．

## SVDとは
特異値分解(singular value decomposition: SVD)は、数学的にはM×N(M行N列)の行列を分解する方法の一つです。
コンピュータサイエンスでは、行列で表現される特徴(情報検索の分野では、例えば文書毎の単語の出現頻度の行列など)を可能な限り損ねること無く次元を圧縮するために利用され、多次元の特徴を扱う画像処理、自然言語処理、検索や推薦に応用されています。
他の次元圧縮手法としては、主成分分析(PCA)や非負値行列因子分解(NMF)などがあります。

# NMF (non-negative matrix factorization) 非負値行列因子分解
https://abicky.net/2010/03/25/101719/
NMFとは非負値行列を2つの非負値行列に分解するアルゴリズムです．
こうすることで，もとの行列が持つ潜在的要素を明確に示すことができるというものです．
https://qiita.com/kusano_t/items/4c0429778613bb4a336d


# 最尤推定(法) maximum likelihood estimation
    統計学において、与えられたデータからそれが従う確率分布の母数を点推定する方法
    ロジスティック回帰は、最尤推定を使用した回帰
    ベイズ推定でデータ数が十分多いなど事後分布が事前分布の影響を受けない時は最尤推定と同じ
    ＝＞最小二乗法などで推定される関数の周辺に一定の確率で分布するとして推定する感じ
    解法にEMアルゴリズムがある

# 混合ガウス分布
    k-meansの時は、各データはどこかのクラスタ１つに所属していました。なので、r1=(0,1,0)r1=(0,1,0)のように0-1の指示変数できっちりと分けていました。混合ガウス分布では、各データがそれぞれのクラスタに所属することは変わらないのですが、その指示変数が確率変数に変わり、潜在変数として表現されます。


# IoU: IoU は Intersection(領域の共通部分) over Union(領域の和集合)の略です。over は「割り算する」という意味です。2つの領域が「どれくらい重なっているか」を表す指標です。
https://mathwords.net/iou

# ベイジアンネットワーク
学術的にはグラフィカルモデルの一種で、確率変数がノード、ノード間の関係が矢線に対応する有向非循環グラフ（DAG）によって表わしたもの。
複数の事象に対する事前の確信が新しい証拠によって変化する様子を表すことができるのですが、このはたらきがベイズ的、すなわち「ベイジアン」ネットワークと呼ばれるゆえん
https://www.synergy-marketing.co.jp/blog/introduction-bayesian-network

# マルコフネットワーク
    ベイジアンネットワークのリンクから矢印を無くしたものがマルコフネットワーク
    ベイジアンネットワークより表現力が弱いものの推論をする時の計算が容易になるという特徴があり、音声や画像処理の分野で用いられます。）

# ナイーブベイズ(単純分類器)
    ベイズの定理を適用することに基づいた単純な確率分類器。テキスト分類に頻繁に用いられている。パラメータ推定には最尤法が使われる。
    ナイーブは「ばからしいほど単純」という意味です。親が1つで子は複数ありお互いに独立している、という最も単純な形のベイジアンネットワークをナイーブベイズと呼びます。一時期スパムフィルタに使われたことで有名になりました。単純とはいえ馬鹿にできない分類性能を持つのですが、これもベイジアンネットワークというには単純すぎるということになります。多段の親子関係や、子同士の相互作用まで考慮するような複雑なモデルでこそ、ベイジアンネットワークの本領が発揮できます。

# 決定木
    葉と根を利用した予測モデル。
    データマイニングでよく利用され、葉が分類、枝がその分類に至るまでの特徴の集合。

# K-平均(法)
    距離ベースのクラスタアルゴリズムで事前に決められた数のクラスタにデータを割り振る。
    ＝＞教師無し学習で代表点からの距離でクラスタリングし、その結果の重心に代表点を移動しクラスタリングを繰り返す
    k近傍法は教師あり学習

# SVM
    Support Vector Machine。２値分類器。
    座標上にサンプル値をプロットし、正値/負例の集合からもっとも距離が大きくなる識別面

# BERT(Bidirectional Encoder Representations from Transformer)
マスクによる穴埋め問題で双方向の学習を実現した。Attentionが全て？WORD2VECと比べ「記号空間から一回り大きい表現空間から情報を掴んだことがBERTが汎用性を獲得した背景」

# バンディットアルゴリズム
- 探索と活用を効率的に行い、一定期間での利益を最大化するアルゴリズム
- （強化学習の文脈では）エージェントが効率的な学習を行うために用いられるアルゴリズム
- 単純のため、以下では値は2値＝＞当たりかハズレか、クリックされたか否か、購入されたか否か

# 正解率で正しい場合はある？
＝＞クラスに偏りが無ければOK。でもマクロ・マイクロF値の方が無難？

# マイクロ平均とマクロ平均の違い？
＝＞マイクロ平均は, まずカテゴリーを無視して全体の適合率と再現率を計算し, それらを用いてF値を計算することにより算出される. マクロ平均はまず各カテゴリーのF値を計算し, それらF値の単純平均を計算することにより算出される. マイクロ平均は, 事例数の大きいカテゴリーに大きな影響を受け, マクロ平均は事例数の小さいカテゴリーに大きな影響を受ける.

# 調和平均：
同じ距離を違う速度で往復した時の平均速度。コストを加味した効率の平均？「調和平均は同じ課題を異なる効率でこなす場合の平均効率であり，適合率も再現率も適合文書を得るための効率」。電気抵抗の合成抵抗の逆数(1/ R) を、抵抗1個あたり平均値 (n / R) に置き換えると調和平均

# F値の由来は？ E値(error)の次がFってだけ？
http://d.hatena.ne.jp/sleepy_yoshi/20110410/p1

# 相乗平均（幾何平均）：ピタゴラスの平均。三角形の残りの一辺の求め方。二乗して足して平方根？

# 相加平均（算術平均）：

# 帰納と演繹：
回帰などデータによる機械学習は帰納法。理論により処理する場合は演繹法。

期待値：平均のこと. E()で表される。ちなみに統計ではmeanが算術平均。avarageは中央値など含めたアバウトな使われ方をするため。

Mean Absolute Error (MAE):平均絶対誤差とも言います。RMSE と MAE は、ともによく使われる誤差の指標です。RMSE はルートの中身で二乗しているので、MAE よりも外れ値（大きなズレ）を、より大きな誤差として扱う傾向があります。

Root mean squared error (RMSE):標準偏差っぽい式です。平均平方二乗誤差、RMS Error、RMSD（Root Mean Square Deviation）などとも呼ばれることがあります。

MSE（Mean Squared Error）:平均二乗誤差とも言います。分散っぽい式です。
https://mathwords.net/rmsemae

MAPE(Mean absolute percentage error):平均絶対パーセント誤差、平均絶対誤差率
https://qiita.com/japanesebonobo/items/ad51cbbf36236b023df0

ロジット関数はロジスティック関数の逆関数

カオス理論：リアプノフ指数を用いて検証した。リアプノフ指数が正であれば、カオス系の特徴である初期値鋭敏性をもつことになり、そのシムテムにおける予測が非常に困難になるという。

コールドスタート問題


------------------
●用語

VAE：変分オートエンコーダ

尤度(ゆうど) : 尤(もっ)もらしさ

機械学習(machine learning)： 機械学習とは、大量の学習データを機械に読み込ませ、分類や判断と言った推論のためのルールを機械に作らせようという仕組みです。そのプロセスは、大きく「学習」と「推論」の２つに分けることができます。

学習(learning): ニューラル・ネットワークの教育課程はトレーニング(training)？learningは広義の学習でtrainingは狭義？
推論(inference):

パターン認識: 音声認識、画像認識、空間認識など纏めた言い方？　自然情報処理のひとつ。画像・音声などの雑多な情報を含むデータの中から、一定の規則や意味を持つ対象を選別して取り出す処理である。「パターン認識はエンジニアリングに起源を持ち、機械学習はコンピュータサイエンスの領域で発達してきましたが、同じ分野の２つの側面ともみなせる。」「パターン認識は工学を起源とするが、機械学習は計算機科学の分野から生じている。しかし、これらの研究活動内容は、同じ分野を2つの側面から見たものとみなせ、両分野とも10年間に大きく発展した。」

Formal Method: 形式手法（Formal Method）数学的な概念を用いて対象を表現する。具体的には、主に「集合」と、集合や集合の要素同士の「関係」を用いて対象を表現する。そして複数の集合や関係に対して何らかの操作をした場合に、それらの関係が「成り立つ」もしくは「成り立たない」という言明を「論理命題」の形で表す。このように「論理」を数学的に扱う学問分野は「数理論理学」と呼ばれ、形式手法は主にその中で研究されてきた。

GEMM(General Matrix Multiply):正方行列同士の積算

テンソル：  テンソル（英: tensor, 独: Tensor）とは、線形的な量または線形的な幾何概念を一般化したもので、基底を選べば、多次元の配列として表現できるようなものである。 しかし、テンソル自身は、特定の座標系によらないで定まる対象である。

テンソル積：数学におけるテンソル積（テンソルせき、英: tensor product）は、線型代数学で重線型性を扱うための線型化を担う概念で、既知のベクトル空間・加群など様々な対象から新たな対象を作り出す操作の一つである。そのようないずれの対象に関しても、テンソル積は最も自由な双線型乗法である。

Define by Run と Define and Run：　前者は柔軟な計算グラフの構築が可能 Chainer等、後者は頭が固いTensorFlow等。

YORO9000: FWではない9000種類以上のオブジェクトカテゴリを検出できる最先端のリアルタイムオブジェクト検出システム

seq2seq(Sequence to Sequence):RNN系のニューラルネットワークを使った文の生成モデルとして、有名なものにsequence to sequence（Seq2Seq）というものがあります。Seq2Seqとは、RNNを用いたEncoderDecoderモデルの一種であり、機械対話や機械翻訳などのモデルとして使用することができます。

word2vec:米グーグルの研究者であるトマス・ミコロフ氏らが提案した自然言語処理の手法。単語をベクトル化して表現するする定量化手法である。例えば日本人が日常的に使う語彙数は数万から数十万といわれるが、Word2Vecでは各単語を200次元くらいの空間内におけるベクトルとして表現する。その結果、今まで分からなかったり精度を向上するのが難しかった単語同士の類似度や、単語間での加算・減算などができるようになり、単語の「意味」を捉えられるようになった。

記号論理学：形態素解析（もっとも小さな文法単位に分割して解析すること）や構文解析（与えられた言語の文法に従って、文法構造を解析すること）によって自然言語文を解析できたとして、文章にひそむ論理構造を見出すことができるでしょうか？言葉で表現される文を論理記号に変換して、その論理関係によって文を捉えるという学問を『記号論理学』と呼び、発祥は１９世紀といわれています。論理学の中で最も基本となるのが、命題論理です。命題論理では、 命題、すなわち、一つ一つの事象を最小単位として扱い、それらと、 論理演算子（論理積や論理和など）を組み合わせて、複雑な命題を構成し、 それらの妥当性（真偽値）を計算したりします。命題論理では、妥当な推論が必ずしも定式化できません。 そこで、命題論理に述語という概念を取り入れ拡張したのが述語論理 です。述語論理では、命題を述語とそれに関わるデータとに分解して 扱います。 ＝＞DL以前の人工知能技術。But Formal Methods(形式手法)には近い？

演繹(deduction)と帰納(induction):演繹法は、「××だから、○○である」という論理を数珠つなぎにしていき、結論を引き出す方法です。帰納法は、多くの観察事項（事実）から類似点をまとめ上げることで、結論を引き出すという論法です。

統計学(statistics)

●画像処理関連
Occlusion:遮蔽. 3次元空間では、上下・左右の他に前後関係があり、手前にある物体が背後にある物体を隠して見えないようにする状態が発生する。これをオクルージョンという。

〇ロボティクス
Robot Operating System (ROS)

〇学習用データセット
PASCAL VOC: アノテーション(四角い枠)付きの画像データセットです。　http://host.robots.ox.ac.uk/pascal/VOC/

COCO - Common Object in Context:セマンティックセグメンテーション情報(いわゆるアノテーションよりも詳しい、画素レベルでの物体認識情報)が付加されたデータセットです。http://cocodataset.org/

CIFAR-10 / CIFAR-100: あのAlexNetのAlex Krizhevsky氏のグループが公開しているデータセットです。10クラス分類と100クラス分類の画像データセットです。http://www.cs.toronto.edu/~kriz/cifar.html

ImageNet: 2009年のCVPRにてプリンストン大学のチームが公開したのがきっかけで発展した、1400万枚超の画像と意味を示すタグのデータセットです。100万枚ほどはバウンディングボックスのアノテーション付きです。http://www.image-net.org/


ーーーーーーーーーーーーー



