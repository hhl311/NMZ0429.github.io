# 推薦システムの評価指標

推薦システムの性能評価に関する研究のまとです。

## 分類・使い分け

### 回帰精度

- ユーザー評価の予測に用いられる
    - 1~5や、1~10の評価の予測
- 分類問題としても解けるが序列性があるので、回帰問題として解ける
    - 連続値で予測した方が、その後のソートなどに使いやすい

### 分類精度（二値分類が主）

- ユーザーの評価・購買・閲覧の有無の予測に用いられる
    - 0か1か、好きか嫌いか、やったかやらないかの予測

### リストの関連性

- 推薦したリストの評価に用いられる
- リストの序列性は考慮されるが、位置が考慮されない古い指標
    - {2,1,3,4,5}と推薦したものと、{1,2,3,5,4}と推薦したものを評価した場合を考える
    - リストでの推薦は、順位が小さいものが相対的に大きく評価されるべきだが、ここで紹介する評価指標は区別がつけられない
    - 一つ後で紹介する、『ランク付けされたリストの関連性』では区別できる（前者が悪く、後者が良く出る）

### ランク付けされたリストの関連性
- 推薦したリストの評価に用いられる
- リストの序列性・位置が考慮される新しい指標
- リストの序盤の序列の間違いに対し、大きなペナルティを与える

### その他
- 推薦システムのリストの評価や、推薦システム全体の評価などに用いられる新しい評価指標
    - アイテム・ユーザーのカバー率
    - 新しいアイテムの推薦や、推薦するが買われないものにペナルティを与えるなど
- 評価するためのデータの入手が難しいのが課題


## 回帰精度

- レーティングの評価に用いられる
- 正確にユーザーのレーティングを予測する能力を持っているか

- テスト用データの評価値と、予測した評価値の差
- 評価閲覧タスク（評価順など）では、評価値そのものを利用者は見る
  - そのような場合、評価値の予測精度のズレを重視すべき

### MAE（平均絶対誤差）

$$
MAE = \frac{1}{N}\sum^{N}_{i=1}|\hat{r_{i}}-r_{i}|
$$

### MSE（平均二乗誤差）

$$
MSE = \frac{1}{N}\sum^{N}_{i=1}(\hat{r_{i}}-r_{i})^{2}
$$

### MAE@{k}, RMSE@{k}

- 実際には、ユーザーに表示されるのはモデルの予測値が高い上位数個
- 上位数アイテム（TopK）を対象にMAE, RMSEを算出し、評価する

$$
MAE@k = \frac{1}{K}\sum^{K}_{i=1}MAE_{i}
$$


## 分類精度（2値分類が主）

- クリック・購入の有無などの２値の評価に用いられる

### 正解率（Acc）

- 利用者の関心が、予測結果とテストデータで一致した割合

- 評価値ならば、高評価か低評価かなど


### 適合率（Precision）と再現率（Recall）、F尺度、ROC

$$
Rrecision = \frac{推薦に含まれる適合アイテムの数}{推薦に含まれる全アイテムの数}
$$

$$
Recall = \frac{検索結果に含まれる適合アイテムの数}{全ての適合アイテムの数}
$$

- 適合アイテムを一つ見つければ良い適合アイテム発見タスクでは、適合率
  - 推薦誤りの少なさ
  - 推薦したアイテム上位n件の内、実際に目的を果たしたものの割合
- 全ての適合アイテムを見つけたい適合アイテム列挙タスクでは、再現率を重視する
  - 推薦漏れの少なさ
  - 目的を果たしたもののうち、推薦で上位n件に含まれている割合
- 総合的な判断をしたい場合はE-measureやF-measureやROC

$$
F = \frac{1}{\frac{\alpha}{Precision}+\frac{1-\alpha}{Recall}}
$$

alpha = 0.5の時はF1値

### n-points Interpolated Precision

- N点のRecallの平均を取る
- 通常、N=11(recall 0.0, 0.1, 0.2, ..., 0.9, 1.0)

$$
nAIP = \frac{1}{N}\sum_{i}InterPrecision_{i}
$$


### ROC AUC

$$
AUC(f) = \frac{\sum_{t_{0}\in D^{0}}\sum_{t_{1}\in D^{1}}1|f(t_{0})<f(t_{1})|}{|D^{0}||D^{1}|}
$$

where

- 1|f(t_{0})<f(t_{1})|は、もしf(t0) < f(t1)ならば1、それ以外ならば0を取る
- D0とは負例のサンプル集合
- D1とは精霊のサンプル集合


### Hit-rate

- 対象となる単項のデータに対する指標（2値データ向け）

1. ユーザーのテストデータから、0でない項目を検索
2. アイテムが推薦リストに含まれるか確かめる
3. Hit-rateを計算する

$$
hit-rate = \frac{num_{hits}}{n}
$$

num_{hits}: 推薦リストの中に含まれていたアイテムの数

n:全ユーザーのアイテム数




## リストの関連性の指標

- 比較的古いランキング指標

- 序列は考慮されているが、ランクの位置そのものは考慮されていない場合が多い

### 順位相関（rank correlation）

### スピアマン順位相関（Spearman's rank）

$$
r_{Spearman}= 1 - \frac{6}{n(n^{2}-1)}\sum^{n}_{i=1}(x_{i}-y{i})^{2}
$$

x_i: 推薦システムによってランク付されたアイテムiの順位

y_i: ユーザーによってランク付されたアイテムiの順位

係数の6/{n*(n^2-1)}の出し方については、二変量正規分布をいじれば出てくる？

- 正しくランク付けされていれば１
- 数値の大小は関係なし


### ケンドールランク（Kendall's rank）

- 二つの項目間の大きさの関係について、ユーザー間の一致を確認する

- 全てのペアがn*(n-1)/2個
- もし、「ユーザーSのAの評価＜ユーザーSのBの評価」かつ「ユーザーTのAの評価＜ユーザーTのBの評価」ならばP+=1
- もし、「ユーザーSのAの評価＞ユーザーSのBの評価」かつ「ユーザーTのAの評価＞ユーザーTのBの評価」ならばP+=1
- それ以外は、Q+=1として、以下を計算する

$$
\tau = \frac{P-Q}{\frac{1}{2}n(n-1)}=\frac{2P}{\frac{1}{2}n(n-1)}-1
$$


### NDMP（Normalized distance-based performance measure）

- C-：矛盾した選好関係の数
  - ユーザーはアイテムAの評価＞アイテムBの評価に対し、推薦がアイテムBの評価＞アイテムAの評価のようになっている数
  - 下の表のD-E
- Cu：推薦とユーザーで同一の選好関係の数
  - 下の表のB-Cなど
- Ci：推薦とユーザーのランキング間のアイテムペアの優先関係数の合計

$$
NDMP = \frac{2C^{-}+C^{u}}{2C^{i}}
$$

| Rank | Recommend | User |
| ---- | --------- | ---- |
| 1    | A         | A    |
| 2    | B         | B    |
| 3    | C         | C    |
| 4    | D         | E    |
| 5    | E         | D    |

$$
上の表のNDMP = \frac{2*1+9}{2*10}=\frac{11}{20}=0.55
$$




## ランク付けされた推薦の評価指標

- 推薦するアイテムの並び方の良さを評価する
- 最も関連するN個のアイテムを正しい順番で予測しているか

- 適合アイテム発見を目的とする場合、推薦システムには意思決定支援の側面が要求される
  - こうした場合は、評価値の大小よりも評価の代償を重視すべき

- アクティブユーザーの完全なランキングを取得すのは難しい
- 大体ここで問題となっているのは、序列に対する重み



### AP（平均適合率）

- ランク付された推薦において、適合アイテムが見つかるたびにその位置での適合率を求め、その算術平均を取った値

$$
AP = \frac{1}{R}\sum^{n}_{r=1}Prec(r)I(r)
$$

- ここで、Prec(r)は適合率Precision、I(r)はr位が適合文書の場合1、そうでない場合は0を取る関数
- しかし、これでは単純に適合したか否かの二値適合性に基づく評価になる
- 実際の推薦の評価では、部分適合・多値適合性に基づく評価指標を用いる

### MAP（Mean Average Precision）

-  MAP@{k}ならば、上位K個のAPの平均を取る

$$
MAP = \frac{1}{N}\sum^{N}_{i=1}AP_{i}
$$


### MRR

- 最初に現れた適合アイテムの順位の逆数の平均をとったもの

1. 全対象ユーザーに対して推薦リストを作成
2. ユーザーが嗜好する適合アイテムが上位何番目で現れたか調べる
3. この順位の逆数を取る（リストに正解が含まれない場合は、逆をとらずに0となる）
4. 全ユーザーで平均を取る

$$
MRR = \frac{1}{N}\sum^{N}_{i=1}\frac{1}{i番目のアイテムの適合順位}
$$

- ランキング下位での順位の差はあまり指標に影響しない


### CG（減損しない累積利得）

- DCGの原型
- 正解したアイテムの利得（スコア）を足し合わせていく

$$
CG = \sum^{n}_{r=1}g(r)
$$

### nDCG（多値適合性に基づく評価指標）

- 適合アイテムの数を加算していく際に、適合アイテムが下位に来るほど減点されるようにする
  - ランキングの上位ほど大きく、下位ほど小さくなるような係数をかけた上で合計する
  - この際の係数を減損利得discounted gainという

- 適合度に応じた価値を利得gainとして考え、ランクが下位になるほどその価値を下げる減損利得discounted gainを導入し、その累積話を正規化した値
- DCG（減損累計利得）は下記
- g(r)はr位における利得
  - ユーザーの実際の評価値などを使用する
  - 1位なら５点、2位なら４点、3位なら３点、4位なら２点、……と適当に重み付けを行っても計算できる

$$
DCG = \sum^{n}_{r=1}\frac{g(r)}{log_{2}(r+1)}
$$

- IDCGは理想的推薦における減損利得の推薦話

- 正規化されたnDCGは以下のようになる

$$
nDCG = \frac{DCG}{IDCG}
$$


### ARHR (Average reciprocal hit rank)

- ２値データ

$$
ARHR = \frac{1}{n}\sum_{i \in Z_{l}}\frac{1}{pos(i)}
$$

Z_{l}: ユーザーの推薦リスト内のゼロでない項目

pos: アイテムのランキングの位置


### Half-life Utility Metric
- アイテムの序列による重要度を半減期パラメータで半減させていく

$$
R_{u} = \sum_{j}\frac{max(r_{u,j}-default, 0)}{2^{(j-1)/(\alpha -1)}}
$$

r_{u, j}: ランクj時点での、ユーザーuの実際のアイテムの評価

default: デフォルトの評価値（通常は評価値の平均）

alpha: 半減期をコントロールするパラメータ


### RBP (Rank biased precision)

- ユーザーモデルを組み込む？

$$
RBP = (1-p)\sum^{n}_{i=1}g_{i}p^{i-1}
$$

gi: ランクiの項目の、ユーザーの好みとの関連性の度合い

p: ランク付されたリストを見ている間のユーザーの持続性をモデル化するパラメータ（通常はクリックログなどから推定）

### ERR (Expected reciprocal rank)

- Cascade user model
  - 上位に存在する項目の関連性に考慮するモデル
  - ユーザーは通常、満足すればそこで検索を終了する
  - この場合、検索を終了した時に下にあるものは全てクリックされない
- カスケードモデルの一つ
- rの位置でユーザーが推薦したランキングの閲覧を止める確率の合計を取る
- この確率は、上位の順位に影響される

$$
ERR = \sum^{n}_{r=1}\zeta(r)P(user~stops~at~position~r) \\
\zeta(r) = \frac{1}{r} ~~~or~~~ \zeta(r) = \frac{1}{log_{2}(r+1)}
$$

もしくは
$$
ERR = \sum^{n}_{r=1}\zeta(r)\prod^{r-1}_{i=1}(1-R_{i})R_{r} \\
R_{i} = \frac{2^{g}-1}{2^{g_{max}}}
$$
g: ユーザーの好みとの関連性の投球

gmax: スケールで最大等級（１〜５の評価がある時の５）


### Q-measure, GAP(Graded Average Precision)

- 平均適合率を拡張したもの



## 精度面以外での、新しい評価指標

- 探索思考ではないが、ユーザーに役立つ指標
  - カバレッジ、学習率、信頼率、信頼性、
- 探索思考で、ユーザーに役立つ指標
  - 推薦がユーザーにとって新しいものか測定する
  - セレンディピティメトリクス
  - ノベルティメトリクス
  - ダイバーシティメトリクス


## 探索思考ではないが有用な指標

### カバレッジ・被覆率（coverage）

- 推薦内容がどの程度のユーザー・アイテムをカバーできているか評価する
  - 協調フィルタリングでは、ユーザーから評価がないアイテムについては、予測を行うことができない
  - コンテンツベースフィルタリングでは、いくつかの特徴量が不足しているアイテムを予測することはできない
- 予測精度とのトレードオフが存在する
  - 牛乳や卵などのほとんどの顧客が購入するものを推薦すれば予測精度は高くなるが、カバレッジは低くなる

### カタログカバレッジ

- 利用可能な全アイテムのうち、１回の推薦でどれくらい多くのアイテムを推薦できたか
- 推薦したアイテムの幅広さを示す

$$
Catalogue Coverage = \frac{|S_{r}|}{|S_{a}|}\\where~S_{a}:アイテム全て\\S_{r}:推薦できるアイテム
$$

- 推薦の範囲を適合アイテムのみに絞った適合カタログカバレッジ

$$
Weighted Catalogue Coverage = \frac{|S_{r}\cap S_{s}|}{|S_{a}|}
$$

### ユーザーカバレッジ

- 全対象ユーザーのうち、直近１回の推薦のうち、どれほど多くのユーザーに対して推薦できたかを示す

$$
PredictionCoverage = \frac{|S_{p}|}{|S_{u}|}\\where~S_{p}:少なくとも１つのアイテムは推薦システムにより推薦できたユーザー\\S_{u}:全てのユーザー
$$


### 学習率

- ユーザーの嗜好が変化した後、どれほど早く推薦システムが適切な推薦を提供できるか
  - コールドスタート問題に関連する
  - システムが素早く適切な推薦を行えないと、ユーザーの満足度は減少する
  - 嗜好が変わったあとの時間
  - ユーザーが推薦受けて始めてから、精度を監視し続ける必要がある


### 確信度

- 推薦にどれほどの確信を持っているか
  - 推薦システムを作成するために、どれほどの数のユーザーやアイテムを使っているか計算する
    - システム構築時のデータのカバー率
  - 協調フィルタリング時に予測で用いるK-NNの類似性を計算する
    - 類似性が低い時、K-NNの距離が遠くなるので、それを監視する？


### 信頼性

- ユーザーの推薦システムへの信頼
- 推薦システムは、ユーザーの嗜好と関心を予測する能力があることを示さなければならない
  - ユーザーがよく知っているアイテムを表示しないと、（信頼を？）得るのが困難
  - 推薦システムを信頼できなければ、ユーザーは推薦システムを使わなくなる
  - 推薦結果が良ければ信頼が増す
  - ユーザーにオンライン評価で直接尋ねる
  - ユーザーの使用頻度で推定する



## 探索思考の指標

- 似た商品ばかり推薦していると、ユーザーは推薦に飽きる
- 既知の商品を推薦されても、意思決定の手助けにはならない

- 評価対象：ユーザーセット、アイテムセット、アイテムのペアなど
- 必要なデータ：評価行列（ユーザー＊アイテム）、オンとロジー（アイテムカテゴリ）、他の新規性・セレンディピティに関するデータセット

### 注意点

- 目新しさだけを推奨すると、ユーザーの満足度は低下する
  - ユーザーは一般的に、既知のものを好む
  - 総合的に評価すると、新規性は利用者の満足度を下げる
  - 斬新なアイテムを推薦する際には注意が必要
- 推薦システムでのユーザーの経験を考慮する
  - 事前に、推薦システムの新奇性を説明する
  - おすすめの新奇項目をそれぞれ説明する


### セレンディピティ（Serendipity）

- 推薦がユーザーに驚きを与えるか否か
  - （ユーザーがアイテムを自身では検索できない）
  - 特定のアイテムを推薦られた時の驚き
  - 他のシステムを利用して計算する？


#### 予想外さ

$$
EXEXP = \frac{1}{|L_{i}|}\sum^{|L_i|}_{i=1}max(P(s_i)-prim(s_i),0)・rel(s_i) \\where ~ s_i:i番目のアイテム \\L_i: 評価済みの推薦リスト \\rel(s_i):ユーザーのアイテムs_iにたいする評価 \\P(s_i): ターゲットシステムで予測された評価 \\prim(s_i): プリミティブシステムで予測された評価
$$

----

$$
UNEXP = RS / PM \\SRDP = \frac{|UNEXP \cap USEFUL|}{N} \\where ~ RS: 推薦システムによるアイテムの集合 \\PM: Primitiveシステムによるアイテムの集合 \\UNEXP: 予想外のアイテムの集合 \\USEFUL: 有用なアイテムの集合
$$


#### エントロピーペースの多様性

- ある推薦システムによるアイテム群は、他の推薦システムも推薦しているかどうか

$$
div_{a,v} = -\sum_{i \in L_{a,u \cap R_{u}}}p_{u,i}logp_{u,i} \\p_{u,i} = \frac{\sum_{a \in A}\delta(a,u,i)}{|A|} \\where ~ A: 推薦した集合 \\a: Targetシステムの評価 \\L_{a,u}: ユーザーuのために推薦システムaにより提供された推薦リスト \\R_u: ユーザーuに関連するアイテム \\\delta :1 if i \in L_{a,u}\cap R_u~and~0~otherwise
$$


### ノベルティ（Novelty）

- 推薦されたアイテムがユーザーにとって未知であるか否か

- Discovery ratio, Presicion of novelty, Item novelty, Temporal novelty, Novelty based on HLU, Long tail metric, Generalized novelty model, Unexpectedness, Entropy-based diversity, Unserendipity, HLU of serendipity

#### 測り方

- 特定アイテムとの付き合い方を聞く
- アイテム間の一般的な人気度や類似度を計算する

- ユーザーXアイテムの評価行列において、0か1か（知っているか知らないか）になる

#### 発見率

- 推薦リストの中に、どれだけユーザーが知らないアイテムがあるか

$$
Discovery = \frac{|D_{i} \cap L_{i}|}{|L_{i}|} \\where~D_{i}:ユーザーiが知らないアイテムのリスト \\L_{i}:ユーザーiに対する推薦リスト
$$

#### ノベルティの精度

$$
Precision(novelty) = \frac{|C_{i} \cap L_{i}|}{|L_{i}|} \\Recall(novelty) = \frac{|C_{i} \cap L_{i}|}{|C_{i}|} \\where~C_{i}:ユーザーiが知らないかつ、好むアイテムのリスト \\L_{i}:ユーザーiに対する推薦リスト
$$


#### アイテムノベルティ

- おすすめアイテムの新規性を測る
  - リスト内の多様性を利用する

$$
Nov_{item}(i) = N(Div(L)-Div(L-(i))) \\= \frac{1}{N-1}\sum_{j \in L}d(i,j) \\where ~ |L| = N \\d(i,j):アイテム間の距離を測る関数 \\Div(): 推薦リストの多様性を測る関数\\
$$

#### テンポラリーノベルティ

- 過去の推薦リストとは異なる推薦を行なっているかを測定する

$$
Nov_{tmp}(L_{i}) = \frac{|\{ x \in L_{i}|x \not= S_{past}\}}{|L_{i}|} \\where~S{past} = \bigcup^{i-1}_{j=1}L_{i}
$$

#### 半減期ベースのノベルティ

- あるアイテムのレーティングの、通常のレーティングからのゲインを、パラメータを元に減少させていく
    - 推薦しているが買われないアイテムは、評価を割り引いていく
- 半減期評価関数

$$
R_{u} = \sum_{j}\frac{max(r_{u,j}-default, 0)}{2^{(j-1)/(\alpha-1)}} \\where~r_{u,j}：ユーザuがjと評価したアイテム \\default:デフォルトの評価値（通常は平均値）\\\alpha : 半減期パラメータ
$$

- NHLU

$$
f(i) = log_{2}(\frac{n}{n_{i}}) \\NHLU_{u} = \sum^{N}_{i}f(i)\frac{max(r_{u,i}-default, 0)}{2^{(i-1)/(\alpha-1)}} \\where~n:ユーザー数 \\n_{i}: アイテムiを選んだユーザー数
$$

#### ロングテールに基づくノベルティ

- 項目を人気に合わせて３つのカテゴリー（HEAD, MID, TAIL）に合わせる
- 上位N個の推薦から、アイテムa_iとa_jを抽出する
- 斬新なアイテムを推薦する能力を、以下の評価マトリクスで評価する

| ai→aj | HEAD | MID  | TAIL |
| ----- | ---- | ---- | ---- |
| HEAD  | 45%  | 55%  | 0%   |
| MID   | 5%   | 70%  | 25%  |
| TAIL  | 0%   | 15%  | 85%  |

- もし、HEAD→HEADが大きければ、ノベルティは低い
- 人気アイテムが入れ替わっているかどうかの指標？


### 多様性（Diversity）

- 様々なタイプのアイテムをユーザーに推薦できるかどうか

- Aggregate diversity, Inter-user diversity, List personalization metric, Gini codfficient, Temporal diversity, Intra-list similarity, Subtopic retrieval, MMR, alpha-nDCG

#### Aggregate diversity

- 推薦システム自体の多様性

- 全てのユーザーに推薦されるアイテムの種類を集計する
- この値が高いことは、推薦システムが利用者に異なる項目を提供していることを示す

$$
Aggdiv = |\bigcup_{n \in U}L_{u}|\\ where~U:ユーザーの集合\\ ~L_{u}: ユーザーuに推薦したアイテムのリスト
$$

- 分母に全アイテム集合をとることで、全体のアイテムのうちどれだけユーザーに推薦できたかを測っている研究も存在
- アイテムの種類→サブカテゴリ・サブトピックなどにすることで、カテゴリ・トピックの多様性を図ることもできる（Subtopic retrieval metric）


#### Inter-user diversity

- ユーザー間の多様性
- ユーザー間で異なる推薦システムを提供する仕組み
- 推薦システムがどれほどユーザー個人個人に合わせられているか(IUD)

$$
d_{u,v} = 1 - \frac{|L_{u} \cap L_{v}|}{N}\\IUD = \frac{1}{N_{U}}\sum_{u,v \in U}d_{u,v}\\where ~ U~(and~V?): ユーザー集合\\ N =|L_{u}|=|L_{v}|,\\ N_{U}: Uの中の２つのペア数\\ L_{u}:ユーザーuに推薦したアイテムリスト
$$

#### List personalization metric

- リストが個人に最適化されているか

$$
p_{b} = \frac{U_{b}}{U} \\I_{b} = log_{2}(\frac{|U|}{U_{b}}) \\Per(L_{i}) = \frac{\sum_{b_{j} \in L_{i}}log\frac{|U|}{|U_{b}|}}{|L_{i}|} \\where ~ p_{b}: アイテムbがユーザーにより選択される確率\\I_{b}: アイテムbの自己エントロピー\\Per(L_{i}): アイテムの個人化の度合い\\U_{b}: アイテムbをせたんkすいたユーザーの数
$$

#### Gini係数

- 所得のアンバランスさの指標などに使われる
- アイテムの推薦頻度がどれだけ偏っているか

- x軸：推薦リスト内の合計頻度の高い順に並べられたx%の割合
- y軸：リスト内の項目のx%の合計頻度

$$
G = \frac{B}{A+B} \\where: A:グラフを書いた時、45％の線と曲線の間の面積 \\B:それ以外の面積（曲線の下の面積）
$$

#### 時間的多様性

- 時刻が違うと、システムが出力する項目が異なる場合の測定

$$
div_{tmp}(L_{1},L_{2},N) = \frac{|(x \in L_{2}|x! \ in L_{1})|}{N} \\where~ N=|L_{1}|=|L_{2}|
$$

#### 推薦リスト内の多様性

- 推薦リストの内容に応じた多様性
- ２つのアイテムの類似度で計算される
  - 通常、特徴ベクトルまたはカテゴリが使われる

$$
Diversity = \sum_{l \in L_{i},m \in L_{i}}\frac{1}{similarity(l,m)} \\where~L_{i}:ユーザーiに推薦したアイテムリスト
$$

$$
ILS(L_{i}) = \frac{\sum_{b_{k} \in L_{i}} \sum_{b_{e} \in L_{i},b_{k} \not= b_{e}} similarity(b_{k},b_{e})}{{}_{|L_{i}|}C_{2}}
$$

#### MMR（Maximal marginal relevance）

- 本来は検索結果に文書を選択するための項目選択法
- この考え方を、推薦結果のリストの評価に利用する

$$
MMR_{List} = \sum_{d_{i} \in L}(\alpha rel(d_{i})-(1-\alpha )max_{d_{i}\in L^{j-1}}sim(d_{i},d_{j})) \\where ~ L: 推薦リスト \\d_{i}: リスト内のアイテム \\rel: そのアイテムをユーザーが好むか否か \\L^{i-1}:J番目までの推薦リスト
$$



## 推薦システムの評価指標の注意点など

- APやnDCGにより、特定のユーザーに対するランク付された推薦を評価できる
- システムを評価する場合、複数ユーザーに対する平均的な良さを評価する必要がある

#### MAP

- ユーザー集合Nに対し、APを算術平均した値

$$
MAP = \frac{1}{N}\sum^{N}_{i=1}AP_{i}
$$

- できるだけ多くのユーザーにある程度適合する推薦を提示したい場合、MAPだけでは不十分

#### GMAP（幾何平均を用いたMAP）

$$
GMAP = \sqrt[n]{\prod_{n}AP_{n}}
$$

便宜上変形すると、以下の形になる

ただし、εは非常に小さい値（10^(-5)程度）
$$
GMAP = exp(\frac{1}{N}\sum^{N}_{i=1}ln(AP_{i}+\epsilon))-\epsilon
$$
一部のユーザーへの推薦で、適合アイテムが一つもない場合、GMAPはMAPより低く評価する

- 上位10位までのように測定長が短い場合、MAPやGMAPの値は不安定なものになりやすい

#### infAP（inferred Average Precision）

- 不完全性なテストコレクションから真の平均適合率を推定する



## 施策に対する評価指標

- アイテムの特定項目分布
  - プッシュしたいカテゴリが存在する場合などは、提供されたアイテムの特定項目に関する分布を基に評価を行うことができる
- NGアイテムの出現頻度
  - 基本的には0を目指す
- 目的のアイテム・カテゴリ等への遷移率
  - ユーザーの遷移先をコントロールしたい場合に計測
- アイテムごとのPV数のジニ係数
  - システムによるアイテムビューが偏らないような仕組みを導入した際に、偏りが是正されるか計測



## interleaving

- 新しいアルゴリズムと古いアルゴリズムを交互に出し、どちらが良い反応か評価する
- https://www.wantedly.com/companies/wantedly/post_articles/155593
- ABテストより10倍以上早く、アルゴリズムの有意差を評価できる
  - 影響が小さい場合は、使える



## 2017年での企業の状況（RecSys 2017 Conference）

![RecSys_2017_Conference](./imgs/2.jpg)
- 推薦システムの学会で、2017年現在での企業で使われている指標は、多い順に以下の通り
1. MRR（序列を考慮した推薦リスト指標・重みは序列の逆比）
2. AUC（分類問題）
3. Precision（適合アイテム1つ発見）
4. nDCG（序列を考慮した推薦リスト指標・重みを調整可能）
5. Recall（適合アイテム全列挙）

- 論文では探索重視の評価指標などは提案されてはいるが、実環境では答えのデータが取得しにくく、現場ではまだあまり使われていない。
- MRRやnDCGは、比較的新しい手法だがよく使われている。


# 参考文献

- 推薦システムについて：http://www.kamishima.net/archive/recsysdoc.pdf
- 評価指標について：https://techblog.nhn-techorus.com/archives/4778
- 検索・推薦の評価方法や評価指標：https://qiita.com/rilmayer/items/4c2c40640c0892e13257
- 評価指標の分類：https://blog.brainpad.co.jp/entry/2017/08/25/140000
- 推薦システムのオフライン評価：http://soc-research.org/wp-content/uploads/2014/11/OfflineTest4RS.pdf



